{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, GlobalAveragePooling1D, Dense, Dropout, BatchNormalization\n",
    "\n",
    "from keras import backend as K\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def precision(y_true, y_pred):\n",
    "#     \"\"\"Precision metric.\"\"\"\n",
    "#     true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "#     predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "#     precision = true_positives / (predicted_positives + K.epsilon())\n",
    "#     return precision\n",
    "\n",
    "# # Register the custom metric function with Keras\n",
    "# tf.keras.metrics.Precision = precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_and_preprocess_data(file_list, sequence_length=500):\n",
    "    data = []\n",
    "    targets = []\n",
    "\n",
    "    for file in file_list:\n",
    "        df = pd.read_csv(file, usecols=[1, 2, 3, 4])\n",
    "        scaler = MinMaxScaler()\n",
    "        df[[\"note\", \"velocity\", \"time\"]] = scaler.fit_transform(\n",
    "            df[[\"note\", \"velocity\", \"time\"]]\n",
    "        )\n",
    "\n",
    "        # Pad the input data if the number of notes is less than the sequence length\n",
    "        if len(df) < sequence_length:\n",
    "            padding = pd.DataFrame(\n",
    "                np.zeros((sequence_length - len(df), 3)),\n",
    "                columns=[\"note\", \"velocity\", \"time\"],\n",
    "            )\n",
    "            df = pd.concat(\n",
    "                [df[[\"note\", \"velocity\", \"time\"]], padding], ignore_index=True\n",
    "            )\n",
    "\n",
    "        data.append(df.iloc[:sequence_length, :-1].values)\n",
    "        targets.append(df[\"anomaly\"].sum())\n",
    "\n",
    "    return np.array(data), np.array(targets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6070, 500, 3) (6070,) (1518, 500, 3) (1518,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "directory = \"./anomalous\"  # Replace with the path to your directory\n",
    "anomalous_file_list = []  # Initialize an empty list\n",
    "\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        anomalous_file_list.append(file_path)\n",
    "\n",
    "sequence_length = 500  # Updated sequence length\n",
    "data, targets = load_and_preprocess_data(anomalous_file_list, sequence_length)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data, targets, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    LSTM(256, activation='tanh', input_shape=(sequence_length, 3), return_sequences=True),\n",
    "    Dropout(0.2),\n",
    "    LSTM(128, activation='tanh', return_sequences=True),\n",
    "    Dropout(0.2),\n",
    "    LSTM(64, activation='tanh'),\n",
    "    BatchNormalization(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(1, activation='linear')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse', run_eagerly=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "190/190 [==============================] - ETA: 0s - loss: 121739.3672\n",
      "Epoch 1: val_loss improved from inf to 84223.17969, saving model to model.h5\n",
      "190/190 [==============================] - 370s 2s/step - loss: 121739.3672 - val_loss: 84223.1797\n",
      "Epoch 2/100\n",
      "190/190 [==============================] - ETA: 0s - loss: 72993.6484\n",
      "Epoch 2: val_loss improved from 84223.17969 to 63464.75391, saving model to model.h5\n",
      "190/190 [==============================] - 376s 2s/step - loss: 72993.6484 - val_loss: 63464.7539\n",
      "Epoch 3/100\n",
      "190/190 [==============================] - ETA: 0s - loss: 67966.3750\n",
      "Epoch 3: val_loss improved from 63464.75391 to 62454.44141, saving model to model.h5\n",
      "190/190 [==============================] - 372s 2s/step - loss: 67966.3750 - val_loss: 62454.4414\n",
      "Epoch 4/100\n",
      "190/190 [==============================] - ETA: 0s - loss: 64510.6289\n",
      "Epoch 4: val_loss did not improve from 62454.44141\n",
      "190/190 [==============================] - 370s 2s/step - loss: 64510.6289 - val_loss: 73930.4688\n",
      "Epoch 5/100\n",
      "190/190 [==============================] - ETA: 0s - loss: 53812.6133\n",
      "Epoch 5: val_loss did not improve from 62454.44141\n",
      "190/190 [==============================] - 370s 2s/step - loss: 53812.6133 - val_loss: 67531.9609\n",
      "Epoch 6/100\n",
      "190/190 [==============================] - ETA: 0s - loss: 43434.9023\n",
      "Epoch 6: val_loss did not improve from 62454.44141\n",
      "190/190 [==============================] - 372s 2s/step - loss: 43434.9023 - val_loss: 104875.0859\n",
      "Epoch 7/100\n",
      "190/190 [==============================] - ETA: 0s - loss: 24659.8672\n",
      "Epoch 7: val_loss improved from 62454.44141 to 13387.12109, saving model to model.h5\n",
      "190/190 [==============================] - 370s 2s/step - loss: 24659.8672 - val_loss: 13387.1211\n",
      "Epoch 8/100\n",
      "190/190 [==============================] - ETA: 0s - loss: 15627.4238\n",
      "Epoch 8: val_loss did not improve from 13387.12109\n",
      "190/190 [==============================] - 366s 2s/step - loss: 15627.4238 - val_loss: 15918.6191\n",
      "Epoch 9/100\n",
      "190/190 [==============================] - ETA: 0s - loss: 12914.8477\n",
      "Epoch 9: val_loss did not improve from 13387.12109\n",
      "190/190 [==============================] - 371s 2s/step - loss: 12914.8477 - val_loss: 51632.6484\n",
      "Epoch 10/100\n",
      "190/190 [==============================] - ETA: 0s - loss: 11202.0742\n",
      "Epoch 10: val_loss did not improve from 13387.12109\n",
      "190/190 [==============================] - 377s 2s/step - loss: 11202.0742 - val_loss: 136161.7656\n",
      "Epoch 11/100\n",
      "190/190 [==============================] - ETA: 0s - loss: 9062.8740\n",
      "Epoch 11: val_loss improved from 13387.12109 to 12619.84180, saving model to model.h5\n",
      "190/190 [==============================] - 378s 2s/step - loss: 9062.8740 - val_loss: 12619.8418\n",
      "Epoch 12/100\n",
      "190/190 [==============================] - ETA: 0s - loss: 9029.6309\n",
      "Epoch 12: val_loss improved from 12619.84180 to 12575.34082, saving model to model.h5\n",
      "190/190 [==============================] - 371s 2s/step - loss: 9029.6309 - val_loss: 12575.3408\n",
      "Epoch 13/100\n",
      "190/190 [==============================] - ETA: 0s - loss: 7675.1367\n",
      "Epoch 13: val_loss did not improve from 12575.34082\n",
      "190/190 [==============================] - 364s 2s/step - loss: 7675.1367 - val_loss: 66550.8281\n",
      "Epoch 14/100\n",
      "190/190 [==============================] - ETA: 0s - loss: 7655.1704\n",
      "Epoch 14: val_loss improved from 12575.34082 to 5459.67676, saving model to model.h5\n",
      "190/190 [==============================] - 375s 2s/step - loss: 7655.1704 - val_loss: 5459.6768\n",
      "Epoch 15/100\n",
      "190/190 [==============================] - ETA: 0s - loss: 7119.5444\n",
      "Epoch 15: val_loss did not improve from 5459.67676\n",
      "190/190 [==============================] - 377s 2s/step - loss: 7119.5444 - val_loss: 29976.4082\n",
      "Epoch 16/100\n",
      "190/190 [==============================] - ETA: 0s - loss: 7055.5142\n",
      "Epoch 16: val_loss did not improve from 5459.67676\n",
      "190/190 [==============================] - 386s 2s/step - loss: 7055.5142 - val_loss: 31102.1426\n",
      "Epoch 17/100\n",
      "190/190 [==============================] - ETA: 0s - loss: 7688.0039\n",
      "Epoch 17: val_loss did not improve from 5459.67676\n",
      "190/190 [==============================] - 383s 2s/step - loss: 7688.0039 - val_loss: 281400.6875\n",
      "Epoch 18/100\n",
      "190/190 [==============================] - ETA: 0s - loss: 7649.9014\n",
      "Epoch 18: val_loss did not improve from 5459.67676\n",
      "190/190 [==============================] - 377s 2s/step - loss: 7649.9014 - val_loss: 19070.3027\n",
      "Epoch 19/100\n",
      "190/190 [==============================] - ETA: 0s - loss: 7078.1665\n",
      "Epoch 19: val_loss did not improve from 5459.67676\n",
      "190/190 [==============================] - 378s 2s/step - loss: 7078.1665 - val_loss: 50690.4883\n",
      "Epoch 20/100\n",
      "190/190 [==============================] - ETA: 0s - loss: 7234.2334\n",
      "Epoch 20: val_loss did not improve from 5459.67676\n",
      "190/190 [==============================] - 372s 2s/step - loss: 7234.2334 - val_loss: 22540.8281\n",
      "Epoch 21/100\n",
      "190/190 [==============================] - ETA: 0s - loss: 7116.2002\n",
      "Epoch 21: val_loss improved from 5459.67676 to 3128.30664, saving model to model.h5\n",
      "190/190 [==============================] - 378s 2s/step - loss: 7116.2002 - val_loss: 3128.3066\n",
      "Epoch 22/100\n",
      "190/190 [==============================] - ETA: 0s - loss: 7215.7666\n",
      "Epoch 22: val_loss did not improve from 3128.30664\n",
      "190/190 [==============================] - 384s 2s/step - loss: 7215.7666 - val_loss: 13412.3447\n",
      "Epoch 23/100\n",
      "190/190 [==============================] - ETA: 0s - loss: 7255.8193\n",
      "Epoch 23: val_loss did not improve from 3128.30664\n",
      "190/190 [==============================] - 378s 2s/step - loss: 7255.8193 - val_loss: 25010.7832\n",
      "Epoch 24/100\n",
      "190/190 [==============================] - ETA: 0s - loss: 7204.2871\n",
      "Epoch 24: val_loss did not improve from 3128.30664\n",
      "190/190 [==============================] - 386s 2s/step - loss: 7204.2871 - val_loss: 14882.0156\n",
      "Epoch 25/100\n",
      "190/190 [==============================] - ETA: 0s - loss: 7245.4946\n",
      "Epoch 25: val_loss did not improve from 3128.30664\n",
      "190/190 [==============================] - 386s 2s/step - loss: 7245.4946 - val_loss: 174641.1250\n",
      "Epoch 26/100\n",
      "190/190 [==============================] - ETA: 0s - loss: 6872.8789\n",
      "Epoch 26: val_loss did not improve from 3128.30664\n",
      "190/190 [==============================] - 382s 2s/step - loss: 6872.8789 - val_loss: 8039.8438\n",
      "Epoch 27/100\n",
      "190/190 [==============================] - ETA: 0s - loss: 7048.7427\n",
      "Epoch 27: val_loss did not improve from 3128.30664\n",
      "190/190 [==============================] - 382s 2s/step - loss: 7048.7427 - val_loss: 72153.1094\n",
      "Epoch 28/100\n",
      "190/190 [==============================] - ETA: 0s - loss: 6881.5459\n",
      "Epoch 28: val_loss improved from 3128.30664 to 1995.49011, saving model to model.h5\n",
      "190/190 [==============================] - 393s 2s/step - loss: 6881.5459 - val_loss: 1995.4901\n",
      "Epoch 29/100\n",
      "190/190 [==============================] - ETA: 0s - loss: 7196.6416\n",
      "Epoch 29: val_loss did not improve from 1995.49011\n",
      "190/190 [==============================] - 390s 2s/step - loss: 7196.6416 - val_loss: 2320.4785\n",
      "Epoch 30/100\n",
      "190/190 [==============================] - ETA: 0s - loss: 6662.5898\n",
      "Epoch 30: val_loss did not improve from 1995.49011\n",
      "190/190 [==============================] - 389s 2s/step - loss: 6662.5898 - val_loss: 3327.4255\n",
      "Epoch 31/100\n",
      "190/190 [==============================] - ETA: 0s - loss: 6519.1353\n",
      "Epoch 31: val_loss did not improve from 1995.49011\n",
      "190/190 [==============================] - 380s 2s/step - loss: 6519.1353 - val_loss: 24271.0742\n",
      "Epoch 32/100\n",
      "190/190 [==============================] - ETA: 0s - loss: 6386.0278\n",
      "Epoch 32: val_loss did not improve from 1995.49011\n",
      "190/190 [==============================] - 380s 2s/step - loss: 6386.0278 - val_loss: 10164.0596\n",
      "Epoch 33/100\n",
      "190/190 [==============================] - ETA: 0s - loss: 6735.1045\n",
      "Epoch 33: val_loss did not improve from 1995.49011\n",
      "190/190 [==============================] - 378s 2s/step - loss: 6735.1045 - val_loss: 18621.5801\n",
      "Epoch 34/100\n",
      "190/190 [==============================] - ETA: 0s - loss: 6379.4858\n",
      "Epoch 34: val_loss did not improve from 1995.49011\n",
      "190/190 [==============================] - 384s 2s/step - loss: 6379.4858 - val_loss: 18297.7109\n",
      "Epoch 35/100\n",
      "190/190 [==============================] - ETA: 0s - loss: 6415.5210\n",
      "Epoch 35: val_loss did not improve from 1995.49011\n",
      "190/190 [==============================] - 384s 2s/step - loss: 6415.5210 - val_loss: 8047.4893\n",
      "Epoch 36/100\n",
      "190/190 [==============================] - ETA: 0s - loss: 7141.6792\n",
      "Epoch 36: val_loss did not improve from 1995.49011\n",
      "190/190 [==============================] - 384s 2s/step - loss: 7141.6792 - val_loss: 19153.1621\n",
      "Epoch 37/100\n",
      "190/190 [==============================] - ETA: 0s - loss: 6751.5249\n",
      "Epoch 37: val_loss did not improve from 1995.49011\n",
      "190/190 [==============================] - 383s 2s/step - loss: 6751.5249 - val_loss: 3377.1733\n",
      "Epoch 38/100\n",
      "190/190 [==============================] - ETA: 0s - loss: 6328.8008\n",
      "Epoch 38: val_loss did not improve from 1995.49011\n",
      "190/190 [==============================] - 384s 2s/step - loss: 6328.8008 - val_loss: 17876.6914\n",
      "48/48 [==============================] - 21s 428ms/step - loss: 1995.4901\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable float object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[89], line 21\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[39m# Load the best model checkpoint and evaluate accuracy\u001b[39;00m\n\u001b[0;32m     20\u001b[0m model\u001b[39m.\u001b[39mload_weights(checkpoint_path)\n\u001b[1;32m---> 21\u001b[0m _, accuracy \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mevaluate(X_test, y_test)\n\u001b[0;32m     22\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mAccuracy: \u001b[39m\u001b[39m{\u001b[39;00maccuracy\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: cannot unpack non-iterable float object"
     ]
    }
   ],
   "source": [
    "\n",
    "# Save the model and print accuracy with the epochs\n",
    "early_stopping_callback = EarlyStopping(monitor=\"val_loss\", patience=10, min_delta=0, mode=\"min\")\n",
    "\n",
    "\n",
    "checkpoint_path = \"model.h5\"\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    checkpoint_path, monitor=\"val_loss\", verbose=1, save_best_only=True, mode=\"min\"\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_test, y_test),\n",
    "    callbacks=[checkpoint_callback, early_stopping_callback],\n",
    ")\n",
    "\n",
    "# Load the best model checkpoint and evaluate accuracy\n",
    "model.load_weights(checkpoint_path)\n",
    "accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_anomalies(model, file, sequence_length=500):\n",
    "    df = pd.read_csv(file, usecols=[1, 2, 3], index_col=False)\n",
    "    print(df.columns)\n",
    "    scaler = MinMaxScaler()\n",
    "    df[[\"note\", \"velocity\", \"time\"]] = scaler.fit_transform(\n",
    "        df[[\"note\", \"velocity\", \"time\"]]\n",
    "    )\n",
    "\n",
    "    # Pad the input data if the number of notes is less than the sequence length\n",
    "    if len(df) < sequence_length:\n",
    "        padding = pd.DataFrame(\n",
    "            np.zeros((sequence_length - len(df), 3)),\n",
    "            columns=[\"note\", \"velocity\", \"time\"],\n",
    "        )\n",
    "        df = pd.concat([df, padding], ignore_index=True)\n",
    "\n",
    "    input_data = df.iloc[:sequence_length, :].values.reshape(1, sequence_length, -1)\n",
    "    print(input_data.shape)\n",
    "    predictions = model.predict(input_data)\n",
    "    predictions[predictions < 0] = 0\n",
    "    return int(np.round(np.sum(predictions)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['note', 'velocity', 'time'], dtype='object')\n",
      "(1, 500, 3)\n",
      "1/1 [==============================] - 0s 68ms/step\n",
      "Number of anomalies: 7\n"
     ]
    }
   ],
   "source": [
    "new_file = \"anomalous/alb_esp2_modified-N-1V-9.csv\"\n",
    "num_anomalies = predict_anomalies(model, new_file)\n",
    "print(f\"Number of anomalies: {num_anomalies}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
